{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\zrack\\fluency_classification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Fluency2</th>\n",
       "      <th>nsyll</th>\n",
       "      <th>npause</th>\n",
       "      <th>dur(s)</th>\n",
       "      <th>phonationtime(s)</th>\n",
       "      <th>speechrate(nsyll/dur)</th>\n",
       "      <th>articulation_rate(nsyll/phonationtime)</th>\n",
       "      <th>ASD(speakingtime/nsyll)</th>\n",
       "      <th>nrFP</th>\n",
       "      <th>tFP(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_33</td>\n",
       "      <td>A2</td>\n",
       "      <td>C1</td>\n",
       "      <td>181</td>\n",
       "      <td>68</td>\n",
       "      <td>167.11</td>\n",
       "      <td>93.72</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.518</td>\n",
       "      <td>55</td>\n",
       "      <td>17.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_33</td>\n",
       "      <td>A2</td>\n",
       "      <td>C1</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>44.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.400</td>\n",
       "      <td>10</td>\n",
       "      <td>2.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_92</td>\n",
       "      <td>B1</td>\n",
       "      <td>B1</td>\n",
       "      <td>221</td>\n",
       "      <td>11</td>\n",
       "      <td>67.11</td>\n",
       "      <td>58.87</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.266</td>\n",
       "      <td>89</td>\n",
       "      <td>17.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_92</td>\n",
       "      <td>B1</td>\n",
       "      <td>B1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>11.00</td>\n",
       "      <td>7.16</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.265</td>\n",
       "      <td>4</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_100</td>\n",
       "      <td>B1</td>\n",
       "      <td>A2</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>34.00</td>\n",
       "      <td>19.89</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.262</td>\n",
       "      <td>22</td>\n",
       "      <td>5.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>audio_2133</td>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>38.33</td>\n",
       "      <td>15.48</td>\n",
       "      <td>1.33</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.303</td>\n",
       "      <td>18</td>\n",
       "      <td>4.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>audio_2144</td>\n",
       "      <td>B1</td>\n",
       "      <td>C1</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>9.85</td>\n",
       "      <td>6.91</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.238</td>\n",
       "      <td>6</td>\n",
       "      <td>1.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>audio_2172</td>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>23.96</td>\n",
       "      <td>17.01</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.254</td>\n",
       "      <td>15</td>\n",
       "      <td>2.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>audio_2081</td>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>116</td>\n",
       "      <td>16</td>\n",
       "      <td>42.36</td>\n",
       "      <td>29.36</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.253</td>\n",
       "      <td>21</td>\n",
       "      <td>5.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>audio_2088</td>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>70</td>\n",
       "      <td>14</td>\n",
       "      <td>27.84</td>\n",
       "      <td>18.96</td>\n",
       "      <td>2.51</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.271</td>\n",
       "      <td>16</td>\n",
       "      <td>3.693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name Fluency Fluency2  nsyll  npause  dur(s)  phonationtime(s)  \\\n",
       "0      audio_33      A2       C1    181      68  167.11             93.72   \n",
       "1      audio_33      A2       C1    110       0   44.00             44.00   \n",
       "2      audio_92      B1       B1    221      11   67.11             58.87   \n",
       "3      audio_92      B1       B1     27       2   11.00              7.16   \n",
       "4     audio_100      B1       A2     76      13   34.00             19.89   \n",
       "..          ...     ...      ...    ...     ...     ...               ...   \n",
       "184  audio_2133      A1       A2     51      12   38.33             15.48   \n",
       "185  audio_2144      B1       C1     29       4    9.85              6.91   \n",
       "186  audio_2172      A2       B2     67      12   23.96             17.01   \n",
       "187  audio_2081      A2       B2    116      16   42.36             29.36   \n",
       "188  audio_2088      A2       B2     70      14   27.84             18.96   \n",
       "\n",
       "     speechrate(nsyll/dur)  articulation_rate(nsyll/phonationtime)  \\\n",
       "0                     1.08                                    1.93   \n",
       "1                     2.50                                    2.50   \n",
       "2                     3.29                                    3.75   \n",
       "3                     2.45                                    3.77   \n",
       "4                     2.24                                    3.82   \n",
       "..                     ...                                     ...   \n",
       "184                   1.33                                    3.30   \n",
       "185                   2.94                                    4.20   \n",
       "186                   2.80                                    3.94   \n",
       "187                   2.74                                    3.95   \n",
       "188                   2.51                                    3.69   \n",
       "\n",
       "     ASD(speakingtime/nsyll)  nrFP  tFP(s)  \n",
       "0                      0.518    55  17.529  \n",
       "1                      0.400    10   2.297  \n",
       "2                      0.266    89  17.050  \n",
       "3                      0.265     4   0.875  \n",
       "4                      0.262    22   5.078  \n",
       "..                       ...   ...     ...  \n",
       "184                    0.303    18   4.123  \n",
       "185                    0.238     6   1.075  \n",
       "186                    0.254    15   2.875  \n",
       "187                    0.253    21   5.009  \n",
       "188                    0.271    16   3.693  \n",
       "\n",
       "[189 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Update these file paths based on the location of your CSV files\n",
    "audio_files_path = \"csv/combined_ratings_for_both_sets.csv\"  # Replace with full path if not in current directory\n",
    "combined_praat_path = \"csv/combined_praat.csv\"         # Replace with full path if not in current directory\n",
    "\n",
    "# Read the CSV files\n",
    "audio_files_df = pd.read_csv(audio_files_path)\n",
    "combined_praat_df = pd.read_csv(combined_praat_path)\n",
    "\n",
    "# Drop the \"Fluency\" column from combined_praat_df\n",
    "if \"Fluency\" in combined_praat_df.columns:\n",
    "    combined_praat_df = combined_praat_df.drop(columns=[\"Fluency\"])\n",
    "\n",
    "# Merge the two dataframes on the \"name\" column\n",
    "merged_df = pd.merge(audio_files_df, combined_praat_df, on=\"name\", how=\"inner\")\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'audio/Human Labeled Proficiency'\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "data=merged_df\n",
    "# Take a random sample of 10% of the lines\n",
    "sampled_data = data\n",
    "\n",
    "# Specify the relevant columns\n",
    "columns = [\n",
    "    \"speechrate(nsyll/dur)\", \"articulation_rate(nsyll/phonationtime)\", \n",
    "    \"npause\", \"dur(s)\", \"ASD(speakingtime/nsyll)\", \"nrFP\"\n",
    "]\n",
    "\n",
    "# Map column names to labels for the formatted output\n",
    "column_labels = {\n",
    "    \"speechrate(nsyll/dur)\": \"Speech Rate\",\n",
    "    \"articulation_rate(nsyll/phonationtime)\": \"Articulation Rate\",\n",
    "    \"npause\": \"Number of Pauses\",\n",
    "    \"dur(s)\": \"Duration\",\n",
    "    \"ASD(speakingtime/nsyll)\": \"ASD (speaking time/nsyll)\",\n",
    "    \"nrFP\": \"Number of Filled Pauses\"\n",
    "}\n",
    "\n",
    "# Format the strings for each sampled row\n",
    "sampled_strings = sampled_data[columns].apply(\n",
    "    lambda row: '\\n'.join([f\"{column_labels[col]}: {row[col]}\" for col in columns]), axis=1\n",
    ").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           name Fluency Fluency2  nsyll  npause  dur(s)  phonationtime(s)  \\\n",
      "0     audio_925      A1       B2     35       1   13.05              8.74   \n",
      "1    audio_1309      A2       C1    103      29   44.91             24.31   \n",
      "2    audio_1195      A1       B1     28       7   11.83              6.30   \n",
      "3    audio_1801      A1       B2     46      17   33.60             12.61   \n",
      "4    audio_1509      B1       A2     21       2    9.67              5.17   \n",
      "..          ...     ...      ...    ...     ...     ...               ...   \n",
      "169  audio_1502      A2       C1     95      15   40.54             25.20   \n",
      "170   audio_629      B2       C1     19       5    8.60              5.83   \n",
      "171  audio_1066      B2       C1     67       7   22.83             16.66   \n",
      "172  audio_1811      B2       C2     47       6   15.21             11.04   \n",
      "173  audio_1243      C1       C1    295      53  106.84             73.84   \n",
      "\n",
      "     speechrate(nsyll/dur)  articulation_rate(nsyll/phonationtime)  \\\n",
      "0                     2.68                                    4.01   \n",
      "1                     2.29                                    4.24   \n",
      "2                     2.37                                    4.44   \n",
      "3                     1.37                                    3.65   \n",
      "4                     2.17                                    4.06   \n",
      "..                     ...                                     ...   \n",
      "169                   2.34                                    3.77   \n",
      "170                   2.21                                    3.26   \n",
      "171                   2.94                                    4.02   \n",
      "172                   3.09                                    4.26   \n",
      "173                   2.76                                    4.00   \n",
      "\n",
      "     ASD(speakingtime/nsyll)  nrFP  tFP(s)  \n",
      "0                      0.250    10   2.161  \n",
      "1                      0.236    16   2.838  \n",
      "2                      0.225     3   0.456  \n",
      "3                      0.274    16   3.096  \n",
      "4                      0.246     2   0.290  \n",
      "..                       ...   ...     ...  \n",
      "169                    0.265    20   5.321  \n",
      "170                    0.307     3   0.720  \n",
      "171                    0.249    14   2.441  \n",
      "172                    0.235    17   2.459  \n",
      "173                    0.250    35   6.793  \n",
      "\n",
      "[174 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the function to sample based on fluency levels\n",
    "def sample_balanced_groups(data, fluency_col_1, fluency_col_2, groups):\n",
    "    \"\"\"\n",
    "    Samples balanced groups from the dataset based on fluency levels.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to sample from.\n",
    "        fluency_col_1 (str): The name of the first fluency column.\n",
    "        fluency_col_2 (str): The name of the second fluency column.\n",
    "        groups (dict): A dictionary defining the fluency groups to sample from.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing balanced samples from each group.\n",
    "    \"\"\"\n",
    "    # Filter rows for each fluency group\n",
    "    sampled_groups = []\n",
    "    for group_name, group_values in groups.items():\n",
    "        group_data = data[\n",
    "            (data[fluency_col_1].isin(group_values)) | (data[fluency_col_2].isin(group_values))\n",
    "        ]\n",
    "        sampled_groups.append(group_data)\n",
    "\n",
    "    # Find the minimum group size\n",
    "    min_group_size = min(len(group) for group in sampled_groups)\n",
    "\n",
    "    # Sample the same number of rows from each group\n",
    "    balanced_samples = pd.concat([group.sample(min_group_size, random_state=42) for group in sampled_groups])\n",
    "    return balanced_samples.reset_index(drop=True)\n",
    "\n",
    "# Define fluency groups\n",
    "fluency_groups = {\n",
    "    \"A1/A2\": [\"A1\", \"A2\"],\n",
    "    \"B1/B2\": [\"B1\", \"B2\"],\n",
    "    \"C1/C2\": [\"C1\", \"C2\"]\n",
    "}\n",
    "\n",
    "# Apply the function to sample balanced groups\n",
    "balanced_sampled_data = sample_balanced_groups(\n",
    "    data=data,\n",
    "    fluency_col_1=\"Fluency\",\n",
    "    fluency_col_2=\"Fluency2\",\n",
    "    groups=fluency_groups\n",
    ")\n",
    "\n",
    "print(balanced_sampled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai = OpenAI(api_key=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Fluency and Fluency2 to the sampled dataframe\n",
    "columns_with_fluency = columns + [\"Fluency\", \"Fluency2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           name Fluency Fluency2  nsyll  npause  dur(s)  phonationtime(s)  \\\n",
      "11   audio_1958      A1       A2     11       2   20.96              3.02   \n",
      "12   audio_1331      A1       A2     13       1   13.66              3.68   \n",
      "13   audio_1918      A1       A1     19       4   10.06              5.34   \n",
      "17   audio_1957      A1       A2     27       7   31.55              8.42   \n",
      "18    audio_725      A1       A2     86      20   38.31             24.96   \n",
      "25   audio_1675      A1       A2     53      19   38.08             16.38   \n",
      "26    audio_728      A1       A2     49       4   17.93             12.51   \n",
      "48   audio_1674      A1       A2     30      12   25.19              9.01   \n",
      "51   audio_1668      A1       A2     33      10   22.22              9.76   \n",
      "53   audio_1238      A1       A2     99      21   44.94             27.24   \n",
      "57   audio_1702      A1       A2     10       3   47.30              2.75   \n",
      "61    audio_788      B1       B2     53       9   22.46             15.68   \n",
      "62   audio_2054      B1       B1     67       9   26.55             19.81   \n",
      "69    audio_657      B1       B2    134      13   43.50             34.25   \n",
      "79    audio_434      B2       B2     33       7   13.55              7.35   \n",
      "81    audio_511      B2       B1    115      25   39.97             23.87   \n",
      "87     audio_92      B1       B1    221      11   67.11             58.87   \n",
      "88    audio_523      B1       B2     77      14   27.46             15.23   \n",
      "91    audio_647      B1       B2     17       3    9.39              4.42   \n",
      "104   audio_431      B1       B2     35       6   13.13              7.34   \n",
      "105   audio_907      B1       B1     26       5   11.74              6.64   \n",
      "118  audio_1140      C1       C2    120      31   51.58             26.26   \n",
      "122  audio_1242      C1       C1    140      21   44.91             32.10   \n",
      "127   audio_155      C1       C1    106      14   38.55             25.89   \n",
      "128  audio_1240      C1       C1    149      22   44.93             31.09   \n",
      "133   audio_180      C1       C2     37       1   11.21              8.70   \n",
      "146  audio_1137      C1       C2    128      23   40.75             26.22   \n",
      "150  audio_1136      C1       C2     78      19   32.58             18.08   \n",
      "158  audio_1624      C1       C2    131      17   42.84             31.14   \n",
      "159  audio_1263      C1       C1    442      58  143.86             99.34   \n",
      "160  audio_1193      C1       C1     88      14   26.00             18.61   \n",
      "162  audio_1708      C1       C2     27       1    9.53              6.03   \n",
      "173  audio_1243      C1       C1    295      53  106.84             73.84   \n",
      "\n",
      "     speechrate(nsyll/dur)  articulation_rate(nsyll/phonationtime)  \\\n",
      "11                    0.52                                    3.64   \n",
      "12                    0.95                                    3.53   \n",
      "13                    1.89                                    3.56   \n",
      "17                    0.86                                    3.21   \n",
      "18                    2.24                                    3.45   \n",
      "25                    1.39                                    3.23   \n",
      "26                    2.73                                    3.92   \n",
      "48                    1.19                                    3.33   \n",
      "51                    1.49                                    3.38   \n",
      "53                    2.20                                    3.63   \n",
      "57                    0.21                                    3.63   \n",
      "61                    2.36                                    3.38   \n",
      "62                    2.52                                    3.38   \n",
      "69                    3.08                                    3.91   \n",
      "79                    2.44                                    4.49   \n",
      "81                    2.88                                    4.82   \n",
      "87                    3.29                                    3.75   \n",
      "88                    2.80                                    5.06   \n",
      "91                    1.81                                    3.85   \n",
      "104                   2.67                                    4.77   \n",
      "105                   2.21                                    3.92   \n",
      "118                   2.33                                    4.57   \n",
      "122                   3.12                                    4.36   \n",
      "127                   2.75                                    4.09   \n",
      "128                   3.32                                    4.79   \n",
      "133                   3.30                                    4.25   \n",
      "146                   3.14                                    4.88   \n",
      "150                   2.39                                    4.31   \n",
      "158                   3.06                                    4.21   \n",
      "159                   3.07                                    4.45   \n",
      "160                   3.38                                    4.73   \n",
      "162                   2.83                                    4.48   \n",
      "173                   2.76                                    4.00   \n",
      "\n",
      "     ASD(speakingtime/nsyll)  nrFP  tFP(s)  \n",
      "11                     0.275     2   0.295  \n",
      "12                     0.283     3   1.072  \n",
      "13                     0.281     6   0.984  \n",
      "17                     0.312     8   1.677  \n",
      "18                     0.290    23   5.208  \n",
      "25                     0.309    22   5.392  \n",
      "26                     0.255    13   2.568  \n",
      "48                     0.300    11   2.688  \n",
      "51                     0.296     9   1.859  \n",
      "53                     0.275    21   5.095  \n",
      "57                     0.275     1   0.152  \n",
      "61                     0.296    14   3.611  \n",
      "62                     0.296    14   4.013  \n",
      "69                     0.256    19   3.257  \n",
      "79                     0.223     6   1.165  \n",
      "81                     0.208    14   1.479  \n",
      "87                     0.266    89  17.050  \n",
      "88                     0.198     4   0.452  \n",
      "91                     0.260     7   0.920  \n",
      "104                    0.210     3   0.649  \n",
      "105                    0.255     3   0.688  \n",
      "118                    0.219    11   2.009  \n",
      "122                    0.229    12   1.695  \n",
      "127                    0.244    11   2.043  \n",
      "128                    0.209     4   0.968  \n",
      "133                    0.235     1   0.112  \n",
      "146                    0.205    10   1.668  \n",
      "150                    0.232     7   1.520  \n",
      "158                    0.238    19   2.725  \n",
      "159                    0.225    21   4.149  \n",
      "160                    0.211    15   1.520  \n",
      "162                    0.223     4   0.792  \n",
      "173                    0.250    35   6.793  \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Define the structured output model for responses\n",
    "class FluencyJudgment(BaseModel):\n",
    "    logic: str\n",
    "    speech_rate: str\n",
    "    articulation_rate: str\n",
    "    number_of_pauses: str\n",
    "    asd: str\n",
    "    number_of_filled_pauses: str\n",
    "    fluency_level: str\n",
    "\n",
    "# Fluency mapping for CEFR levels\n",
    "fluency_mapping = {\n",
    "    \"A1\": \"Beginner\",\n",
    "    \"A2\": \"Beginner\",\n",
    "    \"B1\": \"Intermediate\",\n",
    "    \"B2\": \"Intermediate\",\n",
    "    \"C1\": \"Advanced\",\n",
    "    \"C2\": \"Advanced\"\n",
    "}\n",
    "\n",
    "# Numeric mapping for fluency levels\n",
    "fluency_numeric_mapping = {\"beginner\": 0, \"intermediate\": 1, \"advanced\": 2}\n",
    "numeric_to_fluency = {0: \"beginner\", 1: \"intermediate\", 2: \"advanced\"}\n",
    "\n",
    "# Ensure all fluency judgments and actual ratings are lowercase\n",
    "ai_fluency_judgments = []\n",
    "calculated_fluency_levels = []\n",
    "actual_fluency_1 = []\n",
    "actual_fluency_2 = []\n",
    "category_analysis = []\n",
    "\n",
    "# Function to calculate final fluency level based on category averages\n",
    "def calculate_final_fluency(categories):\n",
    "    numeric_values = [fluency_numeric_mapping[category.lower()] for category in categories]\n",
    "    average_value = round(sum(numeric_values) / len(numeric_values))\n",
    "    return numeric_to_fluency[average_value]\n",
    "\n",
    "# Keep only rows where Fluency and Fluency2 agree after mapping\n",
    "filtered_data = balanced_sampled_data[\n",
    "    balanced_sampled_data.apply(\n",
    "        lambda row: fluency_mapping.get(row['Fluency'], \"unknown\").lower() == fluency_mapping.get(row['Fluency2'], \"unknown\").lower(),\n",
    "        axis=1\n",
    "    )\n",
    "]\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for LLM: [{'input': 'speech_rate: 1.810\\narticulation_rate: 3.850\\nnumber_of_pauses/duration: 0.319\\nasd: 0.260\\nnumber_of_filled_pauses/duration: 0.745', 'fluency_1': 'intermediate', 'fluency_2': 'intermediate'}, {'input': 'speech_rate: 3.300\\narticulation_rate: 4.250\\nnumber_of_pauses/duration: 0.089\\nasd: 0.235\\nnumber_of_filled_pauses/duration: 0.089', 'fluency_1': 'advanced', 'fluency_2': 'advanced'}, {'input': 'speech_rate: 2.730\\narticulation_rate: 3.920\\nnumber_of_pauses/duration: 0.223\\nasd: 0.255\\nnumber_of_filled_pauses/duration: 0.725', 'fluency_1': 'beginner', 'fluency_2': 'beginner'}, {'input': 'speech_rate: 2.210\\narticulation_rate: 3.920\\nnumber_of_pauses/duration: 0.426\\nasd: 0.255\\nnumber_of_filled_pauses/duration: 0.256', 'fluency_1': 'intermediate', 'fluency_2': 'intermediate'}, {'input': 'speech_rate: 2.760\\narticulation_rate: 4.000\\nnumber_of_pauses/duration: 0.496\\nasd: 0.250\\nnumber_of_filled_pauses/duration: 0.328', 'fluency_1': 'advanced', 'fluency_2': 'advanced'}]\n",
      "Speech Rate: 2.83\n",
      "Articulation Rate: 4.48\n",
      "Number of Pauses: 1\n",
      "ASD (speaking time/nsyll): 0.223\n",
      "Number of Filled Pauses: 4\n",
      "Speech Rate: 2.88\n",
      "Articulation Rate: 4.82\n",
      "Number of Pauses: 25\n",
      "ASD (speaking time/nsyll): 0.208\n",
      "Number of Filled Pauses: 14\n",
      "Speech Rate: 3.14\n",
      "Articulation Rate: 4.88\n",
      "Number of Pauses: 23\n",
      "ASD (speaking time/nsyll): 0.205\n",
      "Number of Filled Pauses: 10\n",
      "Speech Rate: 2.8\n",
      "Articulation Rate: 5.06\n",
      "Number of Pauses: 14\n",
      "ASD (speaking time/nsyll): 0.198\n",
      "Number of Filled Pauses: 4\n",
      "Speech Rate: 1.49\n",
      "Articulation Rate: 3.38\n",
      "Number of Pauses: 10\n",
      "ASD (speaking time/nsyll): 0.296\n",
      "Number of Filled Pauses: 9\n",
      "Speech Rate: 2.2\n",
      "Articulation Rate: 3.63\n",
      "Number of Pauses: 21\n",
      "ASD (speaking time/nsyll): 0.275\n",
      "Number of Filled Pauses: 21\n",
      "Speech Rate: 2.67\n",
      "Articulation Rate: 4.77\n",
      "Number of Pauses: 6\n",
      "ASD (speaking time/nsyll): 0.21\n",
      "Number of Filled Pauses: 3\n",
      "Speech Rate: 2.33\n",
      "Articulation Rate: 4.57\n",
      "Number of Pauses: 31\n",
      "ASD (speaking time/nsyll): 0.219\n",
      "Number of Filled Pauses: 11\n",
      "Speech Rate: 2.52\n",
      "Articulation Rate: 3.38\n",
      "Number of Pauses: 9\n",
      "ASD (speaking time/nsyll): 0.296\n",
      "Number of Filled Pauses: 14\n",
      "Speech Rate: 0.52\n",
      "Articulation Rate: 3.64\n",
      "Number of Pauses: 2\n",
      "ASD (speaking time/nsyll): 0.275\n",
      "Number of Filled Pauses: 2\n",
      "Speech Rate: 2.24\n",
      "Articulation Rate: 3.45\n",
      "Number of Pauses: 20\n",
      "ASD (speaking time/nsyll): 0.29\n",
      "Number of Filled Pauses: 23\n",
      "Speech Rate: 3.29\n",
      "Articulation Rate: 3.75\n",
      "Number of Pauses: 11\n",
      "ASD (speaking time/nsyll): 0.266\n",
      "Number of Filled Pauses: 89\n",
      "Speech Rate: 1.39\n",
      "Articulation Rate: 3.23\n",
      "Number of Pauses: 19\n",
      "ASD (speaking time/nsyll): 0.309\n",
      "Number of Filled Pauses: 22\n",
      "Speech Rate: 3.08\n",
      "Articulation Rate: 3.91\n",
      "Number of Pauses: 13\n",
      "ASD (speaking time/nsyll): 0.256\n",
      "Number of Filled Pauses: 19\n",
      "Speech Rate: 2.36\n",
      "Articulation Rate: 3.38\n",
      "Number of Pauses: 9\n",
      "ASD (speaking time/nsyll): 0.296\n",
      "Number of Filled Pauses: 14\n",
      "Speech Rate: 3.32\n",
      "Articulation Rate: 4.79\n",
      "Number of Pauses: 22\n",
      "ASD (speaking time/nsyll): 0.209\n",
      "Number of Filled Pauses: 4\n",
      "Speech Rate: 0.95\n",
      "Articulation Rate: 3.53\n",
      "Number of Pauses: 1\n",
      "ASD (speaking time/nsyll): 0.283\n",
      "Number of Filled Pauses: 3\n",
      "Speech Rate: 1.89\n",
      "Articulation Rate: 3.56\n",
      "Number of Pauses: 4\n",
      "ASD (speaking time/nsyll): 0.281\n",
      "Number of Filled Pauses: 6\n",
      "Speech Rate: 3.07\n",
      "Articulation Rate: 4.45\n",
      "Number of Pauses: 58\n",
      "ASD (speaking time/nsyll): 0.225\n",
      "Number of Filled Pauses: 21\n",
      "Speech Rate: 0.86\n",
      "Articulation Rate: 3.21\n",
      "Number of Pauses: 7\n",
      "ASD (speaking time/nsyll): 0.312\n",
      "Number of Filled Pauses: 8\n",
      "Speech Rate: 2.39\n",
      "Articulation Rate: 4.31\n",
      "Number of Pauses: 19\n",
      "ASD (speaking time/nsyll): 0.232\n",
      "Number of Filled Pauses: 7\n",
      "Speech Rate: 2.75\n",
      "Articulation Rate: 4.09\n",
      "Number of Pauses: 14\n",
      "ASD (speaking time/nsyll): 0.244\n",
      "Number of Filled Pauses: 11\n",
      "Speech Rate: 3.38\n",
      "Articulation Rate: 4.73\n",
      "Number of Pauses: 14\n",
      "ASD (speaking time/nsyll): 0.211\n",
      "Number of Filled Pauses: 15\n",
      "Speech Rate: 3.12\n",
      "Articulation Rate: 4.36\n",
      "Number of Pauses: 21\n",
      "ASD (speaking time/nsyll): 0.229\n",
      "Number of Filled Pauses: 12\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform a train/test split\n",
    "train_data, test_data = train_test_split(filtered_data, test_size=0.7, random_state=42)\n",
    "\n",
    "# Prepare \"examples\" list from the training data for the LLM\n",
    "examples = []\n",
    "for _, row in train_data.iterrows():\n",
    "    # Normalize \"number_of_pauses\" and \"number_of_filled_pauses\" by \"duration\"\n",
    "    duration = row[\"dur(s)\"]  # Assuming the duration column is named \"dur(s)\"\n",
    "    normalized_number_of_pauses = row[\"npause\"] / duration if duration else 0\n",
    "    normalized_number_of_filled_pauses = row[\"nrFP\"] / duration if duration else 0\n",
    "\n",
    "    # Create the input string\n",
    "    line = '\\n'.join([\n",
    "        f\"speech_rate: {row['speechrate(nsyll/dur)']:.3f}\",\n",
    "        f\"articulation_rate: {row['articulation_rate(nsyll/phonationtime)']:.3f}\",\n",
    "        f\"number_of_pauses/duration: {normalized_number_of_pauses:.3f}\",\n",
    "        f\"asd: {row['ASD(speakingtime/nsyll)']:.3f}\",\n",
    "        f\"number_of_filled_pauses/duration: {normalized_number_of_filled_pauses:.3f}\"\n",
    "    ])\n",
    "    fluency1_mapped = fluency_mapping.get(row['Fluency'], \"unknown\").lower()\n",
    "    fluency2_mapped = fluency_mapping.get(row['Fluency2'], \"unknown\").lower()\n",
    "    fluency_actual = {\n",
    "        \"Fluency_1\": fluency1_mapped,\n",
    "        \"Fluency_2\": fluency2_mapped\n",
    "    }\n",
    "    # Append the example to the list\n",
    "    examples.append({\n",
    "        \"input\": line,\n",
    "        \"fluency_1\": fluency1_mapped,\n",
    "        \"fluency_2\": fluency2_mapped\n",
    "    })\n",
    "\n",
    "# Debug print to check the examples\n",
    "print(\"Examples for LLM:\", examples[:5])  # Show the first 5 examples as a sample\n",
    "\n",
    "# Prepare the test data for evaluation\n",
    "sample_data = test_data  # The test data will be used for further processing\n",
    "\n",
    "# Remaining code for processing the test data\n",
    "# Variables to track matches and tau calculations\n",
    "matches_fluency_1 = 0\n",
    "matches_fluency_2 = 0\n",
    "calculated_fluency_levels = []\n",
    "category_analysis = []\n",
    "ai_fluency_judgments = []\n",
    "actual_fluency_1 = []\n",
    "actual_fluency_2 = []\n",
    "category_numeric_values = {name: [] for name in [\"speech_rate\", \"articulation_rate\", \"number_of_pauses\", \"asd\", \"number_of_filled_pauses\"]}\n",
    "\n",
    "# Process each row in the sampled dataset (test data)\n",
    "for _, row in sample_data[columns_with_fluency].iterrows():\n",
    "    # Format the input string\n",
    "    line = '\\n'.join([\n",
    "        f\"{column_labels[col]}: {row[col]}\" \n",
    "        for col in columns if col != 'dur(s)'\n",
    "    ])\n",
    "    fluency1_mapped = fluency_mapping.get(row['Fluency'], \"unknown\").lower()\n",
    "    fluency2_mapped = fluency_mapping.get(row['Fluency2'], \"unknown\").lower()\n",
    "    fluency_actual = {\n",
    "        \"Fluency_1\": fluency1_mapped,\n",
    "        \"Fluency_2\": fluency2_mapped\n",
    "    }\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "    You are an impartial evaluator tasked with assessing the English fluency level of English learners speaking in conversational English. You will be provided with features describing their speech fluency, pausing, and pronunciation.\n",
    "\n",
    "    ### Context:\n",
    "    English fluency is assessed across five key features: speech rate, articulation rate, number of pauses, average syllable duration (ASD), and number of filled pauses. Each feature is categorized as beginner, intermediate, or advanced based on specific criteria.\n",
    "\n",
    "    ### Scoring Criteria:\n",
    "\n",
    "    - **Speech Rate (nsyll/dur):**\n",
    "    - **Beginner**: Very slow, frequent long pauses, unnatural rhythm.\n",
    "    - **Intermediate**: Moderate, some pauses, mostly natural rhythm but occasionally disrupted.\n",
    "    - **Advanced**: Consistently natural rhythm, smooth, and steady.\n",
    "\n",
    "    - **Articulation Rate (nsyll/phonationtime):**\n",
    "    - **Beginner**: Low, with frequent disruptions and interruptions.\n",
    "    - **Intermediate**: Moderate, with occasional disruptions but mostly smooth.\n",
    "    - **Advanced**: High, fluid articulation with minimal interruptions.\n",
    "\n",
    "    - **Average Syllable Duration (ASD) (speakingtime/nsyll):**\n",
    "    - **Beginner**: High, long syllable durations, indicating unnatural rhythm.\n",
    "    - **Intermediate**: Moderate, occasional unevenness in timing.\n",
    "    - **Advanced**: Low, smooth, natural timing for syllables.\n",
    "\n",
    "    ### Evaluation Process:\n",
    "    1. **Analyze the Features**:\n",
    "    - Assign each feature a level: beginner, intermediate, or advanced based on the rubric.\n",
    "    2. **Determine the Overall Fluency Level**:\n",
    "    - If most categories are beginner, classify as \"Beginner.\"\n",
    "    - If most categories are intermediate, classify as \"Intermediate.\"\n",
    "    - If most categories are advanced, classify as \"Advanced.\"\n",
    "    - In case of a tie, prioritize \"speech rate\" and \"articulation rate.\"\n",
    "\n",
    "    ### Examples for Reference:\n",
    "    {examples}\n",
    "\n",
    "\n",
    "    Output Format:\n",
    "\n",
    "    Produce a JSON object with the following structure:\n",
    "        \"logic\": your logic in arriving at your answer,\n",
    "        \"speech_rate\": level,\n",
    "        \"articulation_rate\": level,\n",
    "        \"asd\": level,\n",
    "        \"fluency_level\": \"overall_level\"\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'speech_rate: 1.810\\narticulation_rate: 3.850\\nnumber_of_pauses/duration: 0.319\\nasd: 0.260\\nnumber_of_filled_pauses/duration: 0.745',\n",
       "  'fluency_1': 'intermediate',\n",
       "  'fluency_2': 'intermediate'},\n",
       " {'input': 'speech_rate: 3.300\\narticulation_rate: 4.250\\nnumber_of_pauses/duration: 0.089\\nasd: 0.235\\nnumber_of_filled_pauses/duration: 0.089',\n",
       "  'fluency_1': 'advanced',\n",
       "  'fluency_2': 'advanced'},\n",
       " {'input': 'speech_rate: 2.730\\narticulation_rate: 3.920\\nnumber_of_pauses/duration: 0.223\\nasd: 0.255\\nnumber_of_filled_pauses/duration: 0.725',\n",
       "  'fluency_1': 'beginner',\n",
       "  'fluency_2': 'beginner'},\n",
       " {'input': 'speech_rate: 2.210\\narticulation_rate: 3.920\\nnumber_of_pauses/duration: 0.426\\nasd: 0.255\\nnumber_of_filled_pauses/duration: 0.256',\n",
       "  'fluency_1': 'intermediate',\n",
       "  'fluency_2': 'intermediate'},\n",
       " {'input': 'speech_rate: 2.760\\narticulation_rate: 4.000\\nnumber_of_pauses/duration: 0.496\\nasd: 0.250\\nnumber_of_filled_pauses/duration: 0.328',\n",
       "  'fluency_1': 'advanced',\n",
       "  'fluency_2': 'advanced'},\n",
       " {'input': 'speech_rate: 1.190\\narticulation_rate: 3.330\\nnumber_of_pauses/duration: 0.476\\nasd: 0.300\\nnumber_of_filled_pauses/duration: 0.437',\n",
       "  'fluency_1': 'beginner',\n",
       "  'fluency_2': 'beginner'},\n",
       " {'input': 'speech_rate: 0.210\\narticulation_rate: 3.630\\nnumber_of_pauses/duration: 0.063\\nasd: 0.275\\nnumber_of_filled_pauses/duration: 0.021',\n",
       "  'fluency_1': 'beginner',\n",
       "  'fluency_2': 'beginner'},\n",
       " {'input': 'speech_rate: 2.440\\narticulation_rate: 4.490\\nnumber_of_pauses/duration: 0.517\\nasd: 0.223\\nnumber_of_filled_pauses/duration: 0.443',\n",
       "  'fluency_1': 'intermediate',\n",
       "  'fluency_2': 'intermediate'},\n",
       " {'input': 'speech_rate: 3.060\\narticulation_rate: 4.210\\nnumber_of_pauses/duration: 0.397\\nasd: 0.238\\nnumber_of_filled_pauses/duration: 0.444',\n",
       "  'fluency_1': 'advanced',\n",
       "  'fluency_2': 'advanced'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Detailed Analysis -----\n",
      "Sample 1:\n",
      "AI Fluency: intermediate\n",
      "Calculated Fluency: intermediate\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: intermediate\n",
      "  asd: 0.296\n",
      "Logic: The speech rate of 2.520 indicates a moderate pace, which aligns with the intermediate level. The articulation rate of 3.380 is also moderate, suggesting some smoothness but with occasional disruptions, placing it in the intermediate category as well. The average syllable duration (ASD) of 0.296 is moderate, indicating some unevenness in timing, which again fits the intermediate classification. Since all three features are categorized as intermediate, the overall fluency level is classified as intermediate.\n",
      "Fluency 1 (Actual): intermediate\n",
      "Fluency 2 (Actual): intermediate\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 2:\n",
      "AI Fluency: beginner\n",
      "Calculated Fluency: beginner\n",
      "Categories:\n",
      "  speech_rate: beginner\n",
      "  articulation_rate: intermediate\n",
      "  asd: 0.283\n",
      "Logic: The speech rate of 0.950 is very slow, indicating a beginner level. The articulation rate of 3.530 is moderate, suggesting some smoothness but with potential disruptions, which aligns with an intermediate level. The average syllable duration (ASD) of 0.283 is moderate, indicating occasional unevenness in timing, which also suggests an intermediate level. Since the majority of the features lean towards beginner, the overall fluency level is classified as 'Beginner.'\n",
      "Fluency 1 (Actual): beginner\n",
      "Fluency 2 (Actual): beginner\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 3:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.223\n",
      "Logic: The speech rate of 2.830 indicates a moderate pace, which aligns with the intermediate level. The articulation rate of 4.480 is high and fluid, suggesting advanced fluency. The average syllable duration (ASD) of 0.223 is low, indicating smooth and natural timing, which also suggests advanced fluency. Since the majority of the features are advanced (articulation rate and ASD), the overall fluency level is classified as advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 4:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: advanced\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.225\n",
      "Logic: The speech rate of 3.070 indicates a consistently natural rhythm, which classifies it as advanced. The articulation rate of 4.450 is high and fluid, also classifying it as advanced. The average syllable duration (ASD) of 0.225 is low, indicating smooth and natural timing for syllables, which is classified as advanced. Since all three features are classified as advanced, the overall fluency level is determined to be advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 5:\n",
      "AI Fluency: intermediate\n",
      "Calculated Fluency: intermediate\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: intermediate\n",
      "  asd: 0.266\n",
      "Logic: The speech rate of 3.290 indicates a moderate pace, which aligns with the intermediate level. The articulation rate of 3.750 is also moderate, suggesting some smoothness but with occasional disruptions, placing it in the intermediate category as well. The average syllable duration (ASD) of 0.266 is low, indicating smooth and natural timing for syllables, which is characteristic of advanced fluency. Since the majority of the features are classified as intermediate, the overall fluency level is determined to be intermediate.\n",
      "Fluency 1 (Actual): intermediate\n",
      "Fluency 2 (Actual): intermediate\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 6:\n",
      "AI Fluency: beginner\n",
      "Calculated Fluency: beginner\n",
      "Categories:\n",
      "  speech_rate: beginner\n",
      "  articulation_rate: intermediate\n",
      "  asd: 0.275\n",
      "Logic: The speech rate is very low (0.520), indicating a beginner level due to the unnatural rhythm and frequent long pauses. The articulation rate (3.640) is moderate, suggesting some smoothness but with potential disruptions, which aligns with an intermediate level. The average syllable duration (0.275) is low, indicating a more natural timing for syllables, which is advanced. However, since the speech rate is the most critical factor and it is classified as beginner, the overall fluency level is determined to be beginner.\n",
      "Fluency 1 (Actual): beginner\n",
      "Fluency 2 (Actual): beginner\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 7:\n",
      "AI Fluency: intermediate\n",
      "Calculated Fluency: intermediate\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: intermediate\n",
      "  asd: 0.256\n",
      "Logic: The speech rate of 3.080 indicates a moderate pace, which aligns with the intermediate level. The articulation rate of 3.910 is also moderate, suggesting some smoothness but with occasional disruptions, placing it in the intermediate category as well. The average syllable duration (ASD) of 0.256 is low, indicating smooth and natural timing for syllables, which is characteristic of the advanced level. Since the majority of the features are classified as intermediate, the overall fluency level is determined to be intermediate.\n",
      "Fluency 1 (Actual): intermediate\n",
      "Fluency 2 (Actual): intermediate\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 8:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.208\n",
      "Logic: The speech rate of 2.880 indicates a moderate pace, which aligns with the intermediate level. The articulation rate of 4.820 is high and fluid, suggesting advanced fluency. The average syllable duration (ASD) of 0.208 is low, indicating smooth and natural timing, which also suggests advanced fluency. Since the majority of the features are advanced (articulation rate and ASD), the overall fluency level is classified as advanced.\n",
      "Fluency 1 (Actual): intermediate\n",
      "Fluency 2 (Actual): intermediate\n",
      "Final Match with Actual Fluency: False\n",
      "--------------------------------------------------\n",
      "Sample 9:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.244\n",
      "Logic: The speech rate of 2.750 indicates a moderate pace, which aligns with the intermediate level. The articulation rate of 4.090 is high and fluid, suggesting advanced fluency. The average syllable duration (ASD) of 0.244 is low, indicating smooth and natural timing, which also suggests advanced fluency. Since the majority of the features are classified as advanced (articulation rate and ASD), the overall fluency level is classified as advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 10:\n",
      "AI Fluency: intermediate\n",
      "Calculated Fluency: intermediate\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: intermediate\n",
      "  asd: 0.275\n",
      "Logic: The speech rate of 2.200 is moderate, indicating an intermediate level. The articulation rate of 3.630 is also moderate, suggesting some smoothness but with occasional disruptions, which aligns with the intermediate level. The average syllable duration (ASD) of 0.275 is low, indicating a smoother and more natural timing for syllables, which is characteristic of an advanced level. However, since the speech rate and articulation rate are both intermediate, the overall fluency level is classified as intermediate based on the majority of features.\n",
      "Fluency 1 (Actual): beginner\n",
      "Fluency 2 (Actual): beginner\n",
      "Final Match with Actual Fluency: False\n",
      "--------------------------------------------------\n",
      "Sample 11:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: advanced\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.205\n",
      "Logic: The speech rate of 3.140 indicates a consistent and natural rhythm, which classifies it as advanced. The articulation rate of 4.880 is high and fluid, also classifying as advanced. The average syllable duration (ASD) of 0.205 is low, indicating smooth and natural timing for syllables, which is advanced as well. Since all three features are classified as advanced, the overall fluency level is determined to be advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 12:\n",
      "AI Fluency: beginner\n",
      "Calculated Fluency: beginner\n",
      "Categories:\n",
      "  speech_rate: beginner\n",
      "  articulation_rate: beginner\n",
      "  asd: 0.309\n",
      "Logic: The speech rate of 1.390 is categorized as beginner due to its very slow pace and frequent long pauses. The articulation rate of 3.230 is also categorized as beginner because it indicates low fluidity with frequent disruptions. The average syllable duration (ASD) of 0.309 is categorized as beginner as it shows long syllable durations, indicating unnatural rhythm. Since all three features are classified as beginner, the overall fluency level is determined to be beginner.\n",
      "Fluency 1 (Actual): beginner\n",
      "Fluency 2 (Actual): beginner\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 13:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: advanced\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.209\n",
      "Logic: The speech rate of 3.320 indicates a consistently natural rhythm, which classifies it as advanced. The articulation rate of 4.790 is high and fluid, also classifying it as advanced. The average syllable duration (ASD) of 0.209 is low, indicating smooth and natural timing for syllables, which is again classified as advanced. Since all three features are classified as advanced, the overall fluency level is determined to be advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 14:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.198\n",
      "Logic: The speech rate of 2.800 is moderate, indicating a mostly natural rhythm with some pauses, which classifies it as intermediate. The articulation rate of 5.060 is high and fluid, indicating minimal interruptions, which classifies it as advanced. The average syllable duration (ASD) of 0.198 is low, indicating smooth and natural timing for syllables, which classifies it as advanced. Since the majority of the categories are advanced, the overall fluency level is classified as advanced.\n",
      "Fluency 1 (Actual): intermediate\n",
      "Fluency 2 (Actual): intermediate\n",
      "Final Match with Actual Fluency: False\n",
      "--------------------------------------------------\n",
      "Sample 15:\n",
      "AI Fluency: beginner\n",
      "Calculated Fluency: beginner\n",
      "Categories:\n",
      "  speech_rate: beginner\n",
      "  articulation_rate: beginner\n",
      "  asd: 0.296\n",
      "Logic: The speech rate of 1.490 is categorized as beginner due to its very slow pace and frequent long pauses. The articulation rate of 3.380 is also categorized as beginner because it indicates low fluidity with frequent disruptions. The average syllable duration (ASD) of 0.296 is categorized as intermediate, showing some unevenness but not excessively long syllable durations. Since the majority of the categories are beginner, the overall fluency level is classified as 'Beginner.'\n",
      "Fluency 1 (Actual): beginner\n",
      "Fluency 2 (Actual): beginner\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 16:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: advanced\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.211\n",
      "Logic: The speech rate of 3.380 indicates a consistently natural rhythm, which classifies it as advanced. The articulation rate of 4.730 is high and fluid, also classifying it as advanced. The average syllable duration (ASD) of 0.211 is low, indicating smooth and natural timing for syllables, which is classified as advanced. Since all three features are classified as advanced, the overall fluency level is determined to be advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 17:\n",
      "AI Fluency: beginner\n",
      "Calculated Fluency: beginner\n",
      "Categories:\n",
      "  speech_rate: beginner\n",
      "  articulation_rate: intermediate\n",
      "  asd: 0.281\n",
      "Logic: The speech rate of 1.890 indicates a very slow pace with frequent long pauses, which classifies it as beginner. The articulation rate of 3.560 is moderate, with some disruptions, placing it in the intermediate category. The average syllable duration (ASD) of 0.281 is moderate, indicating occasional unevenness in timing, which also aligns with the intermediate level. Since the majority of the categories are classified as beginner (speech rate) and intermediate (articulation rate and ASD), the overall fluency level is determined by the speech rate, which is beginner. Therefore, the overall fluency level is classified as beginner.\n",
      "Fluency 1 (Actual): beginner\n",
      "Fluency 2 (Actual): beginner\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 18:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.232\n",
      "Logic: The speech rate of 2.390 indicates a moderate pace, which aligns with the intermediate level. The articulation rate of 4.310 is high and fluid, suggesting advanced fluency. The average syllable duration (ASD) of 0.232 is low, indicating smooth and natural timing, which also suggests advanced fluency. Since the majority of the features are classified as advanced (articulation rate and ASD), the overall fluency level is classified as advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 19:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: advanced\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.229\n",
      "Logic: The speech rate of 3.120 indicates a consistent and natural rhythm, which classifies it as advanced. The articulation rate of 4.360 is also high and fluid, indicating minimal interruptions, thus classified as advanced. The average syllable duration (ASD) of 0.229 is low, suggesting smooth and natural timing for syllables, which is classified as advanced. Since all three features are classified as advanced, the overall fluency level is also classified as advanced.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "Sample 20:\n",
      "AI Fluency: advanced\n",
      "Calculated Fluency: advanced\n",
      "Categories:\n",
      "  speech_rate: intermediate\n",
      "  articulation_rate: advanced\n",
      "  asd: 0.219\n",
      "Logic: The speech rate of 2.330 is moderate, indicating an intermediate level. The articulation rate of 4.570 is high and fluid, suggesting an advanced level. The average syllable duration (ASD) of 0.219 is low, indicating smooth and natural timing, which is also advanced. Since the majority of the features are advanced (articulation rate and ASD), the overall fluency level is classified as advanced, despite the speech rate being intermediate.\n",
      "Fluency 1 (Actual): advanced\n",
      "Fluency 2 (Actual): advanced\n",
      "Final Match with Actual Fluency: True\n",
      "--------------------------------------------------\n",
      "----- Final Statistical Summary -----\n",
      "Matches with Fluency 1: 17\n",
      "Matches with Fluency 2: 17\n",
      "Matches with Calculated Fluency: 17\n",
      "Kendall's Tau with Fluency 1: 0.871676664338692\n",
      "Kendall's Tau with Fluency 2: 0.871676664338692\n",
      "Category-wise Kendall's Tau:\n",
      "  speech_rate: 0.5557092130765551\n",
      "  articulation_rate: 0.550773065806269\n",
      "  asd: -0.552235945051653\n",
      "Total Correct Final Matches with Actual Fluency: 17 / 20\n",
      "----- End of Analysis -----\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Check the size of filtered_data\n",
    "if len(test_data) <= 20:\n",
    "    sample_data = test_data  # Use all rows if there are 20 or fewer\n",
    "else:\n",
    "    sample_data = test_data.sample(n=20, random_state=42)  # Random sample of 20 rows\n",
    "\n",
    "# Variables to track matches and tau calculations\n",
    "matches_fluency_1 = 0\n",
    "matches_fluency_2 = 0\n",
    "calculated_fluency_levels = []\n",
    "category_analysis = []\n",
    "ai_fluency_judgments = []\n",
    "actual_fluency_1 = []\n",
    "actual_fluency_2 = []\n",
    "category_numeric_values = {name: [] for name in [\n",
    "    \"speech_rate\", \n",
    "    \"articulation_rate\", \n",
    "    \"asd\"\n",
    "]}\n",
    "\n",
    "# Process each row in the sampled dataset\n",
    "for _, row in sample_data.iterrows():\n",
    "    duration = row['dur(s)']  # Get the duration of the audio\n",
    "\n",
    "    # Normalize only selected categories by duration\n",
    "    normalized_categories = {\n",
    "        \"speech_rate\": row[\"speechrate(nsyll/dur)\"],  # Not normalized\n",
    "        \"articulation_rate\": row[\"articulation_rate(nsyll/phonationtime)\"],  # Not normalized\n",
    "        \"asd\": row[\"ASD(speakingtime/nsyll)\"]  # Not normalized\n",
    "    }\n",
    "\n",
    "    # Add normalized values to category_numeric_values\n",
    "    for category_name, category_value in normalized_categories.items():\n",
    "        category_numeric_values[category_name].append(category_value)\n",
    "\n",
    "    # Create the input string for the LLM\n",
    "    line = '\\n'.join([\n",
    "        f\"{key}: {value:.3f}\"  # Format values to 3 decimal places\n",
    "        for key, value in normalized_categories.items()\n",
    "    ])\n",
    "\n",
    "    fluency1_mapped = fluency_mapping.get(row['Fluency'], \"unknown\").lower()\n",
    "    fluency2_mapped = fluency_mapping.get(row['Fluency2'], \"unknown\").lower()\n",
    "    fluency_actual = {\n",
    "        \"Fluency_1\": fluency1_mapped,\n",
    "        \"Fluency_2\": fluency2_mapped\n",
    "    }\n",
    "\n",
    "    # Use OpenAI's Structured Outputs feature\n",
    "    response = openai.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{line}\"}\n",
    "        ],\n",
    "        response_format=FluencyJudgment,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    response = response.choices[0].message.parsed\n",
    "    ai_judgment = response.fluency_level.lower()\n",
    "    logic_value = response.logic  # Extract logic for printing later\n",
    "    ai_fluency_judgments.append(ai_judgment)\n",
    "    actual_fluency_1.append(fluency_actual[\"Fluency_1\"])\n",
    "    actual_fluency_2.append(fluency_actual[\"Fluency_2\"])\n",
    "\n",
    "    # Analyze if the AI fluency level aligns with categories\n",
    "    correct_categories = [\n",
    "        response.speech_rate,\n",
    "        response.articulation_rate,\n",
    "        normalized_categories[\"asd\"],\n",
    "        response.fluency_level\n",
    "    ]\n",
    "\n",
    "    category_analysis.append({\n",
    "        \"AI Fluency\": ai_judgment,\n",
    "        \"Categories\": correct_categories,\n",
    "        \"Logic\": logic_value,  # Add logic for later analysis\n",
    "        \"Final Match\": ai_judgment == fluency_actual[\"Fluency_1\"] or ai_judgment == fluency_actual[\"Fluency_2\"]\n",
    "    })\n",
    "\n",
    "    # Calculate fluency level based on categorical fluency values only\n",
    "    calculated_fluency = calculate_final_fluency([response.fluency_level])\n",
    "    calculated_fluency_levels.append(calculated_fluency)\n",
    "\n",
    "# Convert fluency levels to numeric values for Kendall's tau\n",
    "ai_fluency_numeric = [fluency_numeric_mapping.get(level, -1) for level in ai_fluency_judgments]\n",
    "calculated_fluency_numeric = [fluency_numeric_mapping.get(level, -1) for level in calculated_fluency_levels]\n",
    "fluency_1_numeric = [fluency_numeric_mapping.get(level, -1) for level in actual_fluency_1]\n",
    "fluency_2_numeric = [fluency_numeric_mapping.get(level, -1) for level in actual_fluency_2]\n",
    "\n",
    "# Calculate Kendall's Tau\n",
    "tau_fluency_1 = (\n",
    "    kendalltau(calculated_fluency_numeric, fluency_1_numeric)[0]\n",
    "    if len(set(calculated_fluency_numeric)) > 1 and len(set(fluency_1_numeric)) > 1\n",
    "    else \"Undefined (Insufficient Variance)\"\n",
    ")\n",
    "\n",
    "tau_fluency_2 = (\n",
    "    kendalltau(calculated_fluency_numeric, fluency_2_numeric)[0]\n",
    "    if len(set(calculated_fluency_numeric)) > 1 and len(set(fluency_2_numeric)) > 1\n",
    "    else \"Undefined (Insufficient Variance)\"\n",
    ")\n",
    "\n",
    "# Calculate Kendall's Tau for each category\n",
    "category_taus = {}\n",
    "for category_name, category_values in category_numeric_values.items():\n",
    "    if len(set(category_values)) > 1 and len(set(fluency_1_numeric)) > 1:\n",
    "        category_taus[category_name] = kendalltau(category_values, fluency_1_numeric)[0]\n",
    "    else:\n",
    "        category_taus[category_name] = \"Undefined (Insufficient Variance)\"\n",
    "\n",
    "\n",
    "# Print detailed analysis for all samples\n",
    "category_names = [\"speech_rate\", \"articulation_rate\", \"asd\"]\n",
    "print(\"----- Detailed Analysis -----\")\n",
    "for i, (analysis, calc_fluency) in enumerate(zip(category_analysis, calculated_fluency_levels)):\n",
    "    ai_fluency = analysis['AI Fluency']\n",
    "    categories = analysis['Categories']\n",
    "    logic = analysis.get('Logic', 'Not Provided')  # Logic value if available\n",
    "    fluency_1 = actual_fluency_1[i]\n",
    "    fluency_2 = actual_fluency_2[i]\n",
    "    final_match = analysis['Final Match']\n",
    "\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"AI Fluency: {ai_fluency}\")\n",
    "    print(f\"Calculated Fluency: {calc_fluency}\")\n",
    "    print(\"Categories:\")\n",
    "    for name, value in zip(category_names, categories[:-1]):  # Skip the redundant fluency_level\n",
    "        if isinstance(value, (int, float)):  # Check if value is numeric\n",
    "            print(f\"  {name}: {value:.3f}\")\n",
    "        else:  # Handle string values\n",
    "            print(f\"  {name}: {value}\")\n",
    "    print(f\"Logic: {logic}\")  # Print logic value\n",
    "    print(f\"Fluency 1 (Actual): {fluency_1}\")\n",
    "    print(f\"Fluency 2 (Actual): {fluency_2}\")\n",
    "    print(f\"Final Match with Actual Fluency: {final_match}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print final statistics\n",
    "print(\"----- Final Statistical Summary -----\")\n",
    "print(f\"Matches with Fluency 1: {sum(ai == human for ai, human in zip(ai_fluency_judgments, actual_fluency_1))}\")\n",
    "print(f\"Matches with Fluency 2: {sum(ai == human for ai, human in zip(ai_fluency_judgments, actual_fluency_2))}\")\n",
    "print(f\"Matches with Calculated Fluency: {sum(calc == human for calc, human in zip(calculated_fluency_levels, actual_fluency_1 + actual_fluency_2))}\")\n",
    "print(f\"Kendall's Tau with Fluency 1: {tau_fluency_1}\")\n",
    "print(f\"Kendall's Tau with Fluency 2: {tau_fluency_2}\")\n",
    "# Print category-wise Kendall's Tau\n",
    "print(\"Category-wise Kendall's Tau:\")\n",
    "for category_name, tau_value in category_taus.items():\n",
    "    print(f\"  {category_name}: {tau_value}\")\n",
    "print(f\"Total Correct Final Matches with Actual Fluency: {sum(1 for a in category_analysis if a['Final Match'])} / {len(category_analysis)}\")\n",
    "print(\"----- End of Analysis -----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New fluency labels assigned and saved to audio/gpt_judged.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'audio/totrain.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the 'Fluency' column but keep everything else\n",
    "columns_to_exclude = [\"Fluency\"]\n",
    "filtered_data = data.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Prepare a list to store rows with GPT-assigned fluency labels\n",
    "output_data = []\n",
    "\n",
    "# Iterate through each row and assign new fluency labels\n",
    "for _, row in filtered_data.iterrows():\n",
    "    # Normalize selected categories for input to the model\n",
    "    normalized_categories = {\n",
    "        \"speech_rate\": row[\"speechrate(nsyll/dur)\"],\n",
    "        \"articulation_rate\": row[\"articulation_rate(nsyll/phonationtime)\"],\n",
    "        \"asd\": row[\"ASD(speakingtime/nsyll)\"]\n",
    "    }\n",
    "\n",
    "    # Create the input string for the LLM\n",
    "    line = '\\n'.join([\n",
    "        f\"{key}: {value:.3f}\"  # Format values to 3 decimal places\n",
    "        for key, value in normalized_categories.items()\n",
    "    ])\n",
    "\n",
    "    # Send input to the AI model\n",
    "    response = openai.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{line}\"}\n",
    "        ],\n",
    "        response_format=FluencyJudgment,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Extract GPT-assigned fluency level\n",
    "    gpt_fluency = response.choices[0].message.parsed.fluency_level\n",
    "\n",
    "    # Append the new row with all original data and GPT fluency\n",
    "    new_row = row.to_dict()\n",
    "    new_row[\"GPT_Fluency\"] = gpt_fluency\n",
    "    output_data.append(new_row)\n",
    "\n",
    "# Convert the list of rows into a new DataFrame\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "output_csv_path = 'audio/gpt_judged.csv'\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"New fluency labels assigned and saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results:\n",
      "CV Accuracy Scores: [0.9        0.98333333 0.95       0.88333333 0.9       ]\n",
      "Mean CV Accuracy: 0.9233\n",
      "Standard Deviation of CV Accuracy: 0.0374\n",
      "CV F1 Macro Scores: [0.90144078 0.98580327 0.95101423 0.88568026 0.88333333]\n",
      "Mean CV F1 Macro: 0.9215\n",
      "Standard Deviation of CV F1 Macro: 0.0404\n",
      "\n",
      "Test Set Results:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    beginner       1.00      0.91      0.95        11\n",
      "intermediate       0.94      0.97      0.95        30\n",
      "    advanced       0.95      0.95      0.95        19\n",
      "\n",
      "    accuracy                           0.95        60\n",
      "   macro avg       0.96      0.94      0.95        60\n",
      "weighted avg       0.95      0.95      0.95        60\n",
      "\n",
      "Test Set Accuracy: 0.9500\n",
      "Test Set F1 Macro Score: 0.9502\n",
      "\n",
      "Feature Importances:\n",
      "speechrate(nsyll/dur): 0.4149\n",
      "articulation_rate(nsyll/phonationtime): 0.3106\n",
      "ASD(speakingtime/nsyll): 0.2745\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the GPT-labeled dataset\n",
    "file_path = 'audio/gpt_judged.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert all labels in 'GPT_Fluency' to lowercase for consistency\n",
    "data[\"GPT_Fluency\"] = data[\"GPT_Fluency\"].str.lower()\n",
    "\n",
    "# Extract features and target\n",
    "features = [\"speechrate(nsyll/dur)\", \"articulation_rate(nsyll/phonationtime)\", \"ASD(speakingtime/nsyll)\"]\n",
    "target = \"GPT_Fluency\"\n",
    "\n",
    "# Prepare the dataset\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Encode target labels (beginner, intermediate, advanced) into numeric values\n",
    "label_mapping = {\"beginner\": 0, \"intermediate\": 1, \"advanced\": 2}\n",
    "y = y.map(label_mapping)\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy and F1 scoring\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation for accuracy and F1 score\n",
    "cv_results = cross_validate(clf, X, y, cv=5, scoring=['accuracy', 'f1_macro'], return_train_score=True)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"CV Accuracy Scores: {cv_results['test_accuracy']}\")\n",
    "print(f\"Mean CV Accuracy: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"Standard Deviation of CV Accuracy: {cv_results['test_accuracy'].std():.4f}\")\n",
    "print(f\"CV F1 Macro Scores: {cv_results['test_f1_macro']}\")\n",
    "print(f\"Mean CV F1 Macro: {cv_results['test_f1_macro'].mean():.4f}\")\n",
    "print(f\"Standard Deviation of CV F1 Macro: {cv_results['test_f1_macro'].std():.4f}\")\n",
    "\n",
    "# Split the data into training and testing sets for further analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the Random Forest Classifier on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate F1 score on test set\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Test Set F1 Macro Score: {f1:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(features, clf.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Results saved to inference_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset to make predictions\n",
    "inference_file_path = 'audio/totrain.csv'  # Path to the dataset for inference\n",
    "inference_data = pd.read_csv(inference_file_path)\n",
    "\n",
    "# Extract features for inference\n",
    "features = [\"speechrate(nsyll/dur)\", \"articulation_rate(nsyll/phonationtime)\", \"ASD(speakingtime/nsyll)\"]\n",
    "X_inference = inference_data[features]\n",
    "\n",
    "# Check if all required features are present\n",
    "missing_features = [feature for feature in features if feature not in inference_data.columns]\n",
    "if missing_features:\n",
    "    raise ValueError(f\"Missing features in dataset: {missing_features}\")\n",
    "\n",
    "# Use the trained Random Forest model to make predictions\n",
    "# Note: Ensure the model has been trained before running this step\n",
    "predicted_labels = clf.predict(X_inference)\n",
    "\n",
    "# Map numeric predictions back to their respective labels\n",
    "label_mapping = {0: \"beginner\", 1: \"intermediate\", 2: \"advanced\"}\n",
    "predicted_fluency = [label_mapping[label] for label in predicted_labels]\n",
    "\n",
    "# Add predictions to the dataset\n",
    "inference_data['Predicted_Fluency'] = predicted_fluency\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_file_path = 'inference_results.csv'\n",
    "inference_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Inference completed. Results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"sylviali/EDEN_ASR_Data\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Filter for English audio samples longer than 8 seconds\n",
    "filtered_data = [sample for sample in dataset if sample['language'] == 'en' and sample['duration'] > 8]\n",
    "\n",
    "# Randomly sample 20 entries\n",
    "random.seed(42)  # For reproducibility\n",
    "sampled_data = random.sample(filtered_data, 20)\n",
    "\n",
    "# Assuming 'model' is your trained Random Forest model\n",
    "predictions = clf.predict(extracted_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Evaluating SVM Model\n",
      "Cross-Validation Results:\n",
      "CV Accuracy Scores: [0.86666667 0.93333333 0.93333333 0.91666667 0.85      ]\n",
      "Mean CV Accuracy: 0.9000\n",
      "Standard Deviation of CV Accuracy: 0.0350\n",
      "CV F1 Macro Scores: [0.86583179 0.93559458 0.92967307 0.92171686 0.80299071]\n",
      "Mean CV F1 Macro: 0.8912\n",
      "Standard Deviation of CV F1 Macro: 0.0506\n",
      "\n",
      "Test Set Results:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    beginner       1.00      0.73      0.84        11\n",
      "intermediate       0.91      0.97      0.94        30\n",
      "    advanced       0.95      1.00      0.97        19\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.95      0.90      0.92        60\n",
      "weighted avg       0.94      0.93      0.93        60\n",
      "\n",
      "Test Set Accuracy: 0.9333\n",
      "Test Set F1 Macro Score: 0.9173\n",
      "\n",
      "Training and Evaluating XGBoost Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zrack\\fluency_classification\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:01:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\zrack\\fluency_classification\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:01:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\zrack\\fluency_classification\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:01:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\zrack\\fluency_classification\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:01:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results:\n",
      "CV Accuracy Scores: [0.88333333 0.96666667 0.98333333 0.88333333 0.91666667]\n",
      "Mean CV Accuracy: 0.9267\n",
      "Standard Deviation of CV Accuracy: 0.0416\n",
      "CV F1 Macro Scores: [0.88401471 0.97183908 0.98580327 0.8794996  0.90337935]\n",
      "Mean CV F1 Macro: 0.9249\n",
      "Standard Deviation of CV F1 Macro: 0.0450\n",
      "\n",
      "Test Set Results:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    beginner       0.91      0.91      0.91        11\n",
      "intermediate       0.97      0.93      0.95        30\n",
      "    advanced       0.95      1.00      0.97        19\n",
      "\n",
      "    accuracy                           0.95        60\n",
      "   macro avg       0.94      0.95      0.94        60\n",
      "weighted avg       0.95      0.95      0.95        60\n",
      "\n",
      "Test Set Accuracy: 0.9500\n",
      "Test Set F1 Macro Score: 0.9442\n",
      "\n",
      "Feature Importances:\n",
      "speechrate(nsyll/dur): 0.2694\n",
      "articulation_rate(nsyll/phonationtime): 0.4380\n",
      "ASD(speakingtime/nsyll): 0.2927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zrack\\fluency_classification\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:01:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\zrack\\fluency_classification\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:01:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the GPT-labeled dataset\n",
    "file_path = 'audio/gpt_judged.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert all labels in 'GPT_Fluency' to lowercase for consistency\n",
    "data[\"GPT_Fluency\"] = data[\"GPT_Fluency\"].str.lower()\n",
    "\n",
    "# Extract features and target\n",
    "features = [\"speechrate(nsyll/dur)\", \"articulation_rate(nsyll/phonationtime)\", \"ASD(speakingtime/nsyll)\"]\n",
    "target = \"GPT_Fluency\"\n",
    "\n",
    "# Prepare the dataset\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Encode target labels (beginner, intermediate, advanced) into numeric values\n",
    "label_mapping = {\"beginner\": 0, \"intermediate\": 1, \"advanced\": 2}\n",
    "y = y.map(label_mapping)\n",
    "\n",
    "# Define models to train\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel='linear', random_state=42, probability=True),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Iterate through models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining and Evaluating {model_name} Model\")\n",
    "\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_results = cross_validate(model, X, y, cv=5, scoring=['accuracy', 'f1_macro'], return_train_score=True)\n",
    "\n",
    "    # Print cross-validation results\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(f\"CV Accuracy Scores: {cv_results['test_accuracy']}\")\n",
    "    print(f\"Mean CV Accuracy: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Standard Deviation of CV Accuracy: {cv_results['test_accuracy'].std():.4f}\")\n",
    "    print(f\"CV F1 Macro Scores: {cv_results['test_f1_macro']}\")\n",
    "    print(f\"Mean CV F1 Macro: {cv_results['test_f1_macro'].mean():.4f}\")\n",
    "    print(f\"Standard Deviation of CV F1 Macro: {cv_results['test_f1_macro'].std():.4f}\")\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate F1 score on test set\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(\"\\nTest Set Results:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))\n",
    "    print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Test Set F1 Macro Score: {f1:.4f}\")\n",
    "\n",
    "    # Feature importance (for XGBoost only)\n",
    "    if model_name == \"XGBoost\":\n",
    "        print(\"\\nFeature Importances:\")\n",
    "        for feature, importance in zip(features, model.feature_importances_):\n",
    "            print(f\"{feature}: {importance:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
